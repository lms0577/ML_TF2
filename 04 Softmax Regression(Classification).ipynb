{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression(Multinomial Classification) - Eager Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "x_train = np.array([\n",
    "    [1., 2., 1., 1.],\n",
    "    [2., 1., 3., 2.],\n",
    "    [3., 1., 3., 4.],\n",
    "    [4., 1., 5., 5.],\n",
    "    [1., 7., 5., 5.],\n",
    "    [1., 2., 5., 6.],\n",
    "    [1., 6., 6., 6.],\n",
    "    [1., 7., 7., 7.]\n",
    "], dtype=np.float32) # 8행 4열\n",
    "\n",
    "y_train = np.array([\n",
    "    [0., 0., 1.],\n",
    "    [0., 0., 1.],\n",
    "    [0., 0., 1.],\n",
    "    [0., 1., 0.],\n",
    "    [0., 1., 0.],\n",
    "    [0., 1., 0.],\n",
    "    [1., 0., 0.],\n",
    "    [1., 0., 0.]\n",
    "], dtype=np.float32) # 8행 3열\n",
    "\n",
    "# test data\n",
    "x_test = np.array([[1.,2.,1.,1.]], dtype=np.float32)\n",
    "y_test = np.array([[0.,0.,1.]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tensorflow Eager\n",
    "### 위 Data를 기준으로 가설의 검증을 통해 Softmax Classification 모델을 만들도록 하겠습니다\n",
    "* Tensorflow data API를 통해 학습시킬 값들을 담는다 (Batch Size는 한번에 학습시킬 Size로 정한다)\n",
    "* features,labels는 실재 학습에 쓰일 Data (연산을 위해 Type를 맞춰준다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 4), (None, 3)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))#.repeat()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위 Data를 기준으로 가설의 검증을 통해 Softmax Classification 모델을 만들도록 하겠습니다\n",
    "* W와 b은 학습을 통해 생성되는 모델에 쓰이는 Wegith와 Bias (초기값을 variable : 0이나 Random값으로 가능 tf.random_normal([2, 1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)  # for reproducibility\n",
    "\n",
    "# 0의 값으로 변수 설정\n",
    "#W = tf.Variable(tf.zeros([8,3]), name='weight')\n",
    "#b = tf.Variable(tf.zeros([3]), name='bias') \n",
    "# 임의의 값으로 변수 설정\n",
    "W = tf.Variable(tf.random.normal((4, 3)), name='weight') # 4행 3열\n",
    "b = tf.Variable(tf.random.normal((3,)), name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Hypothesis using matrix(가설 or 모델)\n",
    "### Softmax 함수를 가설로 선언합니다\n",
    "* Softmax는 tf.nn.softmax(tf.matmul(X, W) + b)와 같습니다\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Softmax(x) & = \\frac{e^{x}}{\\sum _{i=1}^{m}{e^{x_i}}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[6.7622626e-01 5.7505764e-02 2.6626796e-01]\n",
      " [8.9231026e-01 9.3884438e-02 1.3805281e-02]\n",
      " [9.9712002e-01 2.5964240e-03 2.8364777e-04]\n",
      " [9.9732119e-01 2.6285697e-03 5.0274964e-05]\n",
      " [7.1394024e-03 1.4758843e-04 9.9271303e-01]\n",
      " [9.5985550e-01 1.4678108e-02 2.5466355e-02]\n",
      " [4.6822127e-02 1.0678009e-03 9.5211011e-01]\n",
      " [1.6258705e-02 2.7375892e-04 9.8346752e-01]], shape=(8, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 가설\n",
    "def softmax_regression(features):\n",
    "    hypothesis  = tf.nn.softmax(tf.matmul(features, W) + b)\n",
    "    return hypothesis\n",
    "\n",
    "print(softmax_regression(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Cost Function (손실 함수)\n",
    "### 크로스 엔트로피 함수\n",
    "$$\n",
    "\\begin{align}\n",
    "cost(h(x),y) & = -\\sum _{i=1}^{m} y log(h(x))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.992257, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(features, labels):\n",
    "    hypothesis = softmax_regression(features)\n",
    "    cost = -tf.reduce_sum(labels * tf.math.log(hypothesis), axis=1)\n",
    "    cost = tf.reduce_mean(cost)\n",
    "    return cost\n",
    "\n",
    "print(loss_fn(x_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Optimizer (Minimize Cost Function)\n",
    "### Gradient descent\n",
    "$$ W := W-\\alpha \\frac { \\partial  }{ \\partial W } cost(W) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법\n",
    "# tf.GradientTape() 사용\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "def grad(features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(features,labels)\n",
    "    return tape.gradient(loss_value, [W,b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추론한 값 중에 확률이 높은 값의 인덱스를 리턴합니다.\n",
    "* 가설을 통해 실재 값과 비교한 정확도를 측정합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.argmax(hypothesis, 1)\n",
    "    labels = tf.argmax(labels, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 3.2291\n",
      "Iter: 100, Loss: 0.7888\n",
      "Iter: 200, Loss: 0.6915\n",
      "Iter: 300, Loss: 0.6310\n",
      "Iter: 400, Loss: 0.5802\n",
      "Iter: 500, Loss: 0.5330\n",
      "Iter: 600, Loss: 0.4873\n",
      "Iter: 700, Loss: 0.4423\n",
      "Iter: 800, Loss: 0.3975\n",
      "Iter: 900, Loss: 0.3526\n",
      "Iter: 1000, Loss: 0.3078\n",
      "Iter: 1100, Loss: 0.2651\n",
      "Iter: 1200, Loss: 0.2378\n",
      "Iter: 1300, Loss: 0.2260\n",
      "Iter: 1400, Loss: 0.2155\n",
      "Iter: 1500, Loss: 0.2059\n",
      "Iter: 1600, Loss: 0.1970\n",
      "Iter: 1700, Loss: 0.1888\n",
      "Iter: 1800, Loss: 0.1813\n",
      "Iter: 1900, Loss: 0.1743\n",
      "Iter: 2000, Loss: 0.1678\n"
     ]
    }
   ],
   "source": [
    "# 훈련 반복 횟수 설정\n",
    "epoch = 2000\n",
    "for step in range(epoch + 1):\n",
    "    for features, labels  in iter(dataset):\n",
    "        grads = grad(features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
    "        if step % 100 == 0:\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(features,labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict (예측)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train # labels, 실제값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.5121222e-06, 1.2661221e-03, 9.9873036e-01],\n",
       "       [7.8488269e-04, 8.5700996e-02, 9.1351408e-01],\n",
       "       [7.7574995e-08, 1.7224593e-01, 8.2775402e-01],\n",
       "       [8.9749898e-07, 8.4324306e-01, 1.5675601e-01],\n",
       "       [2.7409819e-01, 7.1326292e-01, 1.2638896e-02],\n",
       "       [1.4462650e-01, 8.5536098e-01, 1.2500031e-05],\n",
       "       [7.3665386e-01, 2.6331601e-01, 3.0136525e-05],\n",
       "       [9.1254658e-01, 8.7452896e-02, 5.2975105e-07]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_regression(x_train).numpy() # prediction, 예측값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 트레인 데이터에 대한 예측\n",
    "test_acc = accuracy_fn(softmax_regression(x_train),y_train)\n",
    "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터에 대한 예측\n",
    "test_acc = accuracy_fn(softmax_regression(x_test),y_test)\n",
    "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
